{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fine-tune a LLM\n",
    "\n",
    "### Fine-tuning LLM\n",
    "* Purpose: Fine-tuning adjusts a pre-existing LLM to perform better on a specific task or domain, utilizing a smaller, domain-specific dataset.\n",
    "* Advantages: Improves model performance for context-specific results and reduces training costs.\n",
    "\n",
    "### Steps in Fine-tuning\n",
    "1. Select a  Pre-trained Model: Choose a base model aligned with the desired functionalities. (e.g. Microsoft's Phi-2, Meta's LLaMA)\n",
    "2. Gather Relevant Dataset: Collect a relevant dataset, with necessary labels or structure. This part is many times considered the real challenge.\n",
    "3. Preprocess Dataset: Clean the data and split it into training, validation, and test sets.\n",
    "4. Fine-tuning: Adjust the model on the prepared dataset to specialize for the specific task.\n",
    "5. Further tailor the model in such a way that it becomes more attuned to the specific requirements, language, and knowledge of the task or domain it will be used for.\n",
    "\n",
    "### Fine-tuning methods\n",
    "\n",
    "* Full Fine-tuning: Updates all model weights to enhance performance across various tasks.\n",
    "* Parameter Efficient Fine-Tuning (PEFT): Updates only a subset of parameters, reducing memory requirements and preserving learned information.\n",
    "\n",
    "### Popular Fine-tuning methods: LoRA and QLoRA \n",
    "\n",
    "#### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Here's how it works:\n",
    "* Weight Matrix Approximation: Instead of updating the entire weight matrix of the pre-trained model (which can be very very large and resource-intensive), LoRA focuses on modifying smaller matrices. These smaller matrices are designed to approximate changes to the larger weight matrix effectively. \n",
    "* LoRA Adapters: The smaller matrices, known as \"LoRA adapters,\" are inserted into the model. These adapters capture the necessary adjustments to the model's weights to adapt it to the specific task or domain.\n",
    "* Efficiency and Performance: By fine-tuning only these smaller matrices, you can achieve similar performance improvements as full model fine-tuning but with significantly less computational cost. \n",
    "\n",
    "Here's a simple example that can be helpful to understand LoRA better:\n",
    "\n",
    "Imagine you have a 6 x 6 large weight matrix W, which is the entire weight matrix of the pre-trained model. (In real LLMs, W has billions of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 4, 7, 6, 8, 2],\n",
       "       [7, 4, 7, 6, 6, 2],\n",
       "       [4, 2, 7, 6, 3, 6],\n",
       "       [4, 3, 9, 3, 4, 8],\n",
       "       [4, 6, 4, 1, 4, 1],\n",
       "       [6, 5, 9, 2, 4, 9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.randint(1, 10, size=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LoRA, instead of fine-tuning all the elements of W, we approximate changes to W using two smaller matrices, A and B. Let's say A is 6x2 matrix and B is a 2x6 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "[[0.67149334 0.43698063]\n",
      " [0.76943609 0.13184095]\n",
      " [0.74918086 0.69805827]\n",
      " [0.74757621 0.01147748]\n",
      " [0.14243444 0.28975049]\n",
      " [0.17845079 0.60621866]]\n",
      "B: \n",
      "[[0.52789592 0.62018729 0.07316205 0.6132555  0.48515602 0.10829241]\n",
      " [0.86703269 0.96251119 0.59437175 0.50047613 0.52704569 0.64197135]]\n"
     ]
    }
   ],
   "source": [
    "A = np.random.random((6,2))\n",
    "B = np.random.random((2,6))\n",
    "print(f\"A: \\n{A}\")\n",
    "print(f\"B: \\n{B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the matrices A and B in LoRA is part of the fine-tuning process. Initially, A and B are randomly initialized and then updated through training to better approximate the changes needed in the large weight matrix W for the specific task.\n",
    "\n",
    "The product of A and B, which we'll call C, is a 6x6 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: \n",
      "[[0.73335508 0.83705038 0.30885677 0.63049536 0.55608779 0.35324668]\n",
      " [0.52049258 0.60409287 0.13465606 0.53784417 0.44278276 0.1679622 ]\n",
      " [1.00072885 1.13652134 0.46971773 0.80880078 0.7313782  0.52926401]\n",
      " [0.40459378 0.47468447 0.0615161  0.46419943 0.36874026 0.08832505]\n",
      " [0.3264137  0.36722411 0.1826403  0.2323619  0.22181467 0.20143608]\n",
      " [0.61981483 0.69416515 0.37337507 0.4128339  0.40608141 0.40849987]]\n"
     ]
    }
   ],
   "source": [
    "C = np.dot(A, B)\n",
    "print(f\"C: \\n{C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LoRA, the original large matrix W is not directly modified during fine-tuning. Instead, the smaller matrices A and B are find-tuned, and their product C is used to adjust W during the model's forward pass. \n",
    "\n",
    "So, the model uses an effectice weight matrix W' which is the sume of the original W and the low-rank approximation C:\n",
    "\n",
    "$$\n",
    "W' = W + C\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
